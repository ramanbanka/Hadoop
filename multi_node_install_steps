-----------------------------------------------------------------MASTER END
1. Install Java

    sudo apt-get update
    sudo apt-get install default-jdk
    java -version

2. Disable IPv6

    sudo apt-get install vim

    sudo vim /etc/sysctl.conf

    # disable ipv6
    net.ipv6.conf.all.disable_ipv6 = 1
    net.ipv6.conf.default.disable_ipv6 = 1
    net.ipv6.conf.lo.disable_ipv6 = 1

    cat /proc/sys/net/ipv6/conf/all/disable_ipv6 ... (should return zero)

3. Adding a dedicated Hadoop User

    sudo addgroup hadoop
    sudo adduser --ingroup hadoop hduser

4. Install SSH

    sudo apt-get install ssh

5. Give hduser Sudo Permission

    sudo adduser hduser sudo

6. Setup SSH Certificates

    su hduser
    ssh-keygen -t rsa -P ""
    cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys

    ssh localhost

7. Install Hadoop

    su hduser
    wget http://mirrors.sonic.net/apache/hadoop/common/hadoop-2.6.0/hadoop-2.6.0.tar.gz
    tar xvzf hadoop-2.6.0.tar.gz
	mv hadoop-2.6.0 hadoop
    sudo cp -r hadoop /home/hduser/

8. Set up the Configuration files

    a. vim ~/.bashrc

        #HADOOP VARIABLES START
        export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64
        export HADOOP_INSTALL=/home/hduser/hadoop
        export PATH=$PATH:$HADOOP_INSTALL/bin
        export PATH=$PATH:$HADOOP_INSTALL/sbin
        export HADOOP_MAPRED_HOME=$HADOOP_INSTALL
        export HADOOP_COMMON_HOME=$HADOOP_INSTALL
        export HADOOP_HDFS_HOME=$HADOOP_INSTALL
        export YARN_HOME=$HADOOP_INSTALL
        export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_INSTALL/lib/native
        export HADOOP_OPTS="-Djava.library.path=$HADOOP_INSTALL/lib"
        #HADOOP VARIABLES END

    b.  vim /home/hduser/hadoop/etc/hadoop/hadoop-env.sh

        export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64

    c.  sudo mkdir -p /home/hduser/tmp
        vim /home/hduser/hadoop/etc/hadoop/core-site.xml

        <property>
            <name>hadoop.tmp.dir</name>
                <value>/home/hduser/tmp</value>
                    <description>A base for other temporary directories.</description>
            </property>

        <property>
            <name>fs.default.name</name>
            <value>hdfs://master:54310</value>
            <description>The name of the default file system.  A URI whose
            scheme and authority determine the FileSystem implementation.  The
            uri's scheme determines the config property (fs.SCHEME.impl) naming
            the FileSystem implementation class.  The uri's authority is used to
            determine the host, port, etc. for a filesystem.</description>
        </property>

    d. cp /home/hduser/hadoop/etc/hadoop/mapred-site.xml.template /home/hduser/hadoop/etc/hadoop/mapred-site.xml
       vim /home/hduser/hadoop/etc/hadoop/mapred-site.xml

         <property>
            <name>mapred.job.tracker</name>
            <value>master:54311</value>
            <description>The host and port that the MapReduce job tracker runs
            at.  If "local", then jobs are run in-process as a single map
            and reduce task.
            </description>
        </property>

    e. sudo mkdir -p /home/hduser/hadoop_store/hdfs/namenode
       //sudo mkdir -p /home/hduser/hadoop_store/hdfs/datanode
       sudo chown -R hduser:hadoop /home/hduser/hadoop_store
       vim /home/hduser/hadoop/etc/hadoop/hdfs-site.xml

        <property>
            <name>dfs.replication</name>
            <value>2</value>
            <description>Default block replication.
            The actual number of replications can be specified when the file is created.
            The default is used if replication is not specified in create time.
            </description>
        </property>
 
        <property>
            <name>dfs.namenode.name.dir</name>
            <value>file:/home/hduser/hadoop_store/hdfs/namenode</value>
        </property>
 
        <!--property>
            <name>dfs.datanode.data.dir</name>
            <value>file:/home/hduser/hadoop_store/hdfs/datanode</value>
        </property-->
	f.  vim /home/hadoopuser/hadoop/etc/hadoop/yarn-site.xml
		
		<property>
		 <name>yarn.nodemanager.aux-services</name>
		 <value>mapreduce_shuffle</value>
		</property>
		<property>
		 <name>yarn.resourcemanager.scheduler.address</name>
		 <value>master:8030</value>
		</property> 
		<property>
		 <name>yarn.resourcemanager.address</name>
		 <value>master:8032</value>
		</property>
		<property>
		  <name>yarn.resourcemanager.webapp.address</name>
		  <value>master:8088</value>
		</property>
		<property>
		  <name>yarn.resourcemanager.resource-tracker.address</name>
		  <value>master:8031</value>
		</property>
		<property>
	g. sudo vim /etc/hostname
			
			#machine name
				master

	   sudo service hostname restart
	   #reopen terminal after this

	h. sudo vim /etc/hosts
			#ipv4 machine_name 
			192.168.15.111 master
			192.168.15.102 slave1
	i. vim /home/hadoopuser/hadoop/etc/hadoop/master
			# master machine name
				master

	j. vim /home/hadoopuser/hadoop/etc/hadoop/slaves
			#slave machine names
				slave1
				slave2
	k. # To avoid repeated password prompts
        ssh-copy-id -i ~/.ssh/id_rsa.pub slave1
9. Format Hadoop filesystem

    hadoop namenode -format

10. Starting Hadoop

    su hduser
    sudo chown -R hduser:hadoop /home/hduser/hadoop/
    
    cd /home/hduser/hadoop/sbin
    start-all.sh

11. Testing if it is working 
    
    jps

    netstat -plten | grep java

    http://localhost:50070/

12. Stopping Hadoop

    stop-all.sh



------------------------------------------------------------------SLAVE END

Repeat steps 1-12

Only change is in 

8.e.

       sudo mkdir -p /home/hduser/hadoop_store/hdfs/datanode
       sudo chown -R hduser:hadoop /home/hduser/hadoop_store
       vim /home/hduser/hadoop/etc/hadoop/hdfs-site.xml

        <property>
            <name>dfs.replication</name>
            <value>2</value>
            <description>Default block replication.
            The actual number of replications can be specified when the file is created.
            The default is used if replication is not specified in create time.
            </description>
        </property>
 
        <property>
            <name>dfs.datanode.data.dir</name>
            <value>file:/home/hduser/hadoop_store/hdfs/datanode</value>
        </property>


8.k.

		ssh-copy-id -i ~/.ssh/id_rsa.pub master
